digraph {
	graph [size="151.95,151.95"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140595792196624 [label="
 (1, 1000)" fillcolor=darkolivegreen1]
	140596165862032 [label=AddmmBackward0]
	140596165862080 -> 140596165862032
	140596165674096 [label="fc.bias
 (1000)" fillcolor=lightblue]
	140596165674096 -> 140596165862080
	140596165862080 [label=AccumulateGrad]
	140596165859392 -> 140596165862032
	140596165859392 [label=ViewBackward0]
	140596165855120 -> 140596165859392
	140596165855120 [label=MeanBackward1]
	140596165861072 -> 140596165855120
	140596165861072 [label=ReluBackward0]
	140596165862896 -> 140596165861072
	140596165862896 [label=AddBackward0]
	140596165860976 -> 140596165862896
	140596165860976 [label=CudnnBatchNormBackward0]
	140596165859008 -> 140596165860976
	140596165859008 [label=ConvolutionBackward0]
	140596165863232 -> 140596165859008
	140596165863232 [label=ReluBackward0]
	140596165863376 -> 140596165863232
	140596165863376 [label=CudnnBatchNormBackward0]
	140596165863472 -> 140596165863376
	140596165863472 [label=ConvolutionBackward0]
	140596165863664 -> 140596165863472
	140596165863664 [label=ReluBackward0]
	140596165863808 -> 140596165863664
	140596165863808 [label=CudnnBatchNormBackward0]
	140596165863904 -> 140596165863808
	140596165863904 [label=ConvolutionBackward0]
	140596165862944 -> 140596165863904
	140596165862944 [label=ReluBackward0]
	140596165864192 -> 140596165862944
	140596165864192 [label=AddBackward0]
	140596165864288 -> 140596165864192
	140596165864288 [label=CudnnBatchNormBackward0]
	140596165864432 -> 140596165864288
	140596165864432 [label=ConvolutionBackward0]
	140596165864624 -> 140596165864432
	140596165864624 [label=ReluBackward0]
	140596165864768 -> 140596165864624
	140596165864768 [label=CudnnBatchNormBackward0]
	140596165864864 -> 140596165864768
	140596165864864 [label=ConvolutionBackward0]
	140596165865056 -> 140596165864864
	140596165865056 [label=ReluBackward0]
	140596165865200 -> 140596165865056
	140596165865200 [label=CudnnBatchNormBackward0]
	140596165865296 -> 140596165865200
	140596165865296 [label=ConvolutionBackward0]
	140596165864240 -> 140596165865296
	140596165864240 [label=ReluBackward0]
	140596165865584 -> 140596165864240
	140596165865584 [label=AddBackward0]
	140596165865680 -> 140596165865584
	140596165865680 [label=CudnnBatchNormBackward0]
	140596165865824 -> 140596165865680
	140596165865824 [label=ConvolutionBackward0]
	140596165866016 -> 140596165865824
	140596165866016 [label=ReluBackward0]
	140596165866160 -> 140596165866016
	140596165866160 [label=CudnnBatchNormBackward0]
	140596165866256 -> 140596165866160
	140596165866256 [label=ConvolutionBackward0]
	140596165866448 -> 140596165866256
	140596165866448 [label=ReluBackward0]
	140596165866592 -> 140596165866448
	140596165866592 [label=CudnnBatchNormBackward0]
	140596165866688 -> 140596165866592
	140596165866688 [label=ConvolutionBackward0]
	140596165866880 -> 140596165866688
	140596165866880 [label=ReluBackward0]
	140596165867024 -> 140596165866880
	140596165867024 [label=AddBackward0]
	140596165867120 -> 140596165867024
	140596165867120 [label=CudnnBatchNormBackward0]
	140596165867264 -> 140596165867120
	140596165867264 [label=ConvolutionBackward0]
	140596165867456 -> 140596165867264
	140596165867456 [label=ReluBackward0]
	140596165867600 -> 140596165867456
	140596165867600 [label=CudnnBatchNormBackward0]
	140596165867696 -> 140596165867600
	140596165867696 [label=ConvolutionBackward0]
	140596165867888 -> 140596165867696
	140596165867888 [label=ReluBackward0]
	140596165868032 -> 140596165867888
	140596165868032 [label=CudnnBatchNormBackward0]
	140596165868128 -> 140596165868032
	140596165868128 [label=ConvolutionBackward0]
	140596165867072 -> 140596165868128
	140596165867072 [label=ReluBackward0]
	140596165868416 -> 140596165867072
	140596165868416 [label=AddBackward0]
	140596165868512 -> 140596165868416
	140596165868512 [label=CudnnBatchNormBackward0]
	140596165868656 -> 140596165868512
	140596165868656 [label=ConvolutionBackward0]
	140596165868848 -> 140596165868656
	140596165868848 [label=ReluBackward0]
	140596165868992 -> 140596165868848
	140596165868992 [label=CudnnBatchNormBackward0]
	140596165869088 -> 140596165868992
	140596165869088 [label=ConvolutionBackward0]
	140596165869280 -> 140596165869088
	140596165869280 [label=ReluBackward0]
	140596165869424 -> 140596165869280
	140596165869424 [label=CudnnBatchNormBackward0]
	140596165869520 -> 140596165869424
	140596165869520 [label=ConvolutionBackward0]
	140596165868464 -> 140596165869520
	140596165868464 [label=ReluBackward0]
	140596158628144 -> 140596165868464
	140596158628144 [label=AddBackward0]
	140596158628240 -> 140596158628144
	140596158628240 [label=CudnnBatchNormBackward0]
	140596158628384 -> 140596158628240
	140596158628384 [label=ConvolutionBackward0]
	140596158628576 -> 140596158628384
	140596158628576 [label=ReluBackward0]
	140596158628720 -> 140596158628576
	140596158628720 [label=CudnnBatchNormBackward0]
	140596158628816 -> 140596158628720
	140596158628816 [label=ConvolutionBackward0]
	140596158629008 -> 140596158628816
	140596158629008 [label=ReluBackward0]
	140596158629152 -> 140596158629008
	140596158629152 [label=CudnnBatchNormBackward0]
	140596158629248 -> 140596158629152
	140596158629248 [label=ConvolutionBackward0]
	140596158628192 -> 140596158629248
	140596158628192 [label=ReluBackward0]
	140596158629536 -> 140596158628192
	140596158629536 [label=AddBackward0]
	140596158629632 -> 140596158629536
	140596158629632 [label=CudnnBatchNormBackward0]
	140596158629776 -> 140596158629632
	140596158629776 [label=ConvolutionBackward0]
	140596158629968 -> 140596158629776
	140596158629968 [label=ReluBackward0]
	140596158630112 -> 140596158629968
	140596158630112 [label=CudnnBatchNormBackward0]
	140596158630208 -> 140596158630112
	140596158630208 [label=ConvolutionBackward0]
	140596158630400 -> 140596158630208
	140596158630400 [label=ReluBackward0]
	140596158630544 -> 140596158630400
	140596158630544 [label=CudnnBatchNormBackward0]
	140596158630640 -> 140596158630544
	140596158630640 [label=ConvolutionBackward0]
	140596158629584 -> 140596158630640
	140596158629584 [label=ReluBackward0]
	140596158630928 -> 140596158629584
	140596158630928 [label=AddBackward0]
	140596158631024 -> 140596158630928
	140596158631024 [label=CudnnBatchNormBackward0]
	140596158631168 -> 140596158631024
	140596158631168 [label=ConvolutionBackward0]
	140596158631360 -> 140596158631168
	140596158631360 [label=ReluBackward0]
	140596158631504 -> 140596158631360
	140596158631504 [label=CudnnBatchNormBackward0]
	140596158631600 -> 140596158631504
	140596158631600 [label=ConvolutionBackward0]
	140596158631792 -> 140596158631600
	140596158631792 [label=ReluBackward0]
	140596158631936 -> 140596158631792
	140596158631936 [label=CudnnBatchNormBackward0]
	140596158631984 -> 140596158631936
	140596158631984 [label=ConvolutionBackward0]
	140596158630976 -> 140596158631984
	140596158630976 [label=ReluBackward0]
	140596158632368 -> 140596158630976
	140596158632368 [label=AddBackward0]
	140596158632416 -> 140596158632368
	140596158632416 [label=CudnnBatchNormBackward0]
	140596158632656 -> 140596158632416
	140596158632656 [label=ConvolutionBackward0]
	140596158632848 -> 140596158632656
	140596158632848 [label=ReluBackward0]
	140596158632992 -> 140596158632848
	140596158632992 [label=CudnnBatchNormBackward0]
	140596158633040 -> 140596158632992
	140596158633040 [label=ConvolutionBackward0]
	140596158633328 -> 140596158633040
	140596158633328 [label=ReluBackward0]
	140596158633472 -> 140596158633328
	140596158633472 [label=CudnnBatchNormBackward0]
	140596158633520 -> 140596158633472
	140596158633520 [label=ConvolutionBackward0]
	140596158633808 -> 140596158633520
	140596158633808 [label=ReluBackward0]
	140596158633952 -> 140596158633808
	140596158633952 [label=AddBackward0]
	140596158634000 -> 140596158633952
	140596158634000 [label=CudnnBatchNormBackward0]
	140596158634240 -> 140596158634000
	140596158634240 [label=ConvolutionBackward0]
	140596158634432 -> 140596158634240
	140596158634432 [label=ReluBackward0]
	140596158634576 -> 140596158634432
	140596158634576 [label=CudnnBatchNormBackward0]
	140596158634624 -> 140596158634576
	140596158634624 [label=ConvolutionBackward0]
	140596158634912 -> 140596158634624
	140596158634912 [label=ReluBackward0]
	140596158635056 -> 140596158634912
	140596158635056 [label=CudnnBatchNormBackward0]
	140596158635104 -> 140596158635056
	140596158635104 [label=ConvolutionBackward0]
	140596158633856 -> 140596158635104
	140596158633856 [label=ReluBackward0]
	140596158635488 -> 140596158633856
	140596158635488 [label=AddBackward0]
	140596158635536 -> 140596158635488
	140596158635536 [label=CudnnBatchNormBackward0]
	140596158635776 -> 140596158635536
	140596158635776 [label=ConvolutionBackward0]
	140596158635968 -> 140596158635776
	140596158635968 [label=ReluBackward0]
	140596158636112 -> 140596158635968
	140596158636112 [label=CudnnBatchNormBackward0]
	140596158636160 -> 140596158636112
	140596158636160 [label=ConvolutionBackward0]
	140596158636448 -> 140596158636160
	140596158636448 [label=ReluBackward0]
	140596158636592 -> 140596158636448
	140596158636592 [label=CudnnBatchNormBackward0]
	140596158636640 -> 140596158636592
	140596158636640 [label=ConvolutionBackward0]
	140596158635296 -> 140596158636640
	140596158635296 [label=ReluBackward0]
	140596158637024 -> 140596158635296
	140596158637024 [label=AddBackward0]
	140596158637072 -> 140596158637024
	140596158637072 [label=CudnnBatchNormBackward0]
	140596158637312 -> 140596158637072
	140596158637312 [label=ConvolutionBackward0]
	140596158637504 -> 140596158637312
	140596158637504 [label=ReluBackward0]
	140596158637648 -> 140596158637504
	140596158637648 [label=CudnnBatchNormBackward0]
	140596158637696 -> 140596158637648
	140596158637696 [label=ConvolutionBackward0]
	140596158637984 -> 140596158637696
	140596158637984 [label=ReluBackward0]
	140596158638128 -> 140596158637984
	140596158638128 [label=CudnnBatchNormBackward0]
	140596158638176 -> 140596158638128
	140596158638176 [label=ConvolutionBackward0]
	140596158636832 -> 140596158638176
	140596158636832 [label=ReluBackward0]
	140596158638560 -> 140596158636832
	140596158638560 [label=AddBackward0]
	140596158638608 -> 140596158638560
	140596158638608 [label=CudnnBatchNormBackward0]
	140596158638848 -> 140596158638608
	140596158638848 [label=ConvolutionBackward0]
	140596158639040 -> 140596158638848
	140596158639040 [label=ReluBackward0]
	140596158639184 -> 140596158639040
	140596158639184 [label=CudnnBatchNormBackward0]
	140596158639232 -> 140596158639184
	140596158639232 [label=ConvolutionBackward0]
	140596158639520 -> 140596158639232
	140596158639520 [label=ReluBackward0]
	140596158639664 -> 140596158639520
	140596158639664 [label=CudnnBatchNormBackward0]
	140596158639712 -> 140596158639664
	140596158639712 [label=ConvolutionBackward0]
	140596158640000 -> 140596158639712
	140596158640000 [label=ReluBackward0]
	140596158640144 -> 140596158640000
	140596158640144 [label=AddBackward0]
	140596158640192 -> 140596158640144
	140596158640192 [label=CudnnBatchNormBackward0]
	140596158640432 -> 140596158640192
	140596158640432 [label=ConvolutionBackward0]
	140596158640624 -> 140596158640432
	140596158640624 [label=ReluBackward0]
	140596158640768 -> 140596158640624
	140596158640768 [label=CudnnBatchNormBackward0]
	140596158640816 -> 140596158640768
	140596158640816 [label=ConvolutionBackward0]
	140596158641104 -> 140596158640816
	140596158641104 [label=ReluBackward0]
	140596158641248 -> 140596158641104
	140596158641248 [label=CudnnBatchNormBackward0]
	140596158641296 -> 140596158641248
	140596158641296 [label=ConvolutionBackward0]
	140596158640048 -> 140596158641296
	140596158640048 [label=ReluBackward0]
	140596158641680 -> 140596158640048
	140596158641680 [label=AddBackward0]
	140596158641728 -> 140596158641680
	140596158641728 [label=CudnnBatchNormBackward0]
	140596158641968 -> 140596158641728
	140596158641968 [label=ConvolutionBackward0]
	140596158642160 -> 140596158641968
	140596158642160 [label=ReluBackward0]
	140596158642304 -> 140596158642160
	140596158642304 [label=CudnnBatchNormBackward0]
	140596158642352 -> 140596158642304
	140596158642352 [label=ConvolutionBackward0]
	140596158642640 -> 140596158642352
	140596158642640 [label=ReluBackward0]
	140596158642784 -> 140596158642640
	140596158642784 [label=CudnnBatchNormBackward0]
	140596158642832 -> 140596158642784
	140596158642832 [label=ConvolutionBackward0]
	140596158641488 -> 140596158642832
	140596158641488 [label=ReluBackward0]
	140596158643216 -> 140596158641488
	140596158643216 [label=AddBackward0]
	140596158643264 -> 140596158643216
	140596158643264 [label=CudnnBatchNormBackward0]
	140596158643504 -> 140596158643264
	140596158643504 [label=ConvolutionBackward0]
	140596158643696 -> 140596158643504
	140596158643696 [label=ReluBackward0]
	140596158643840 -> 140596158643696
	140596158643840 [label=CudnnBatchNormBackward0]
	140596158643888 -> 140596158643840
	140596158643888 [label=ConvolutionBackward0]
	140596158644176 -> 140596158643888
	140596158644176 [label=ReluBackward0]
	140596158333088 -> 140596158644176
	140596158333088 [label=CudnnBatchNormBackward0]
	140596158333136 -> 140596158333088
	140596158333136 [label=ConvolutionBackward0]
	140596158333424 -> 140596158333136
	140596158333424 [label=MaxPool2DWithIndicesBackward0]
	140596158333568 -> 140596158333424
	140596158333568 [label=ReluBackward0]
	140596158333616 -> 140596158333568
	140596158333616 [label=CudnnBatchNormBackward0]
	140596158333760 -> 140596158333616
	140596158333760 [label=ConvolutionBackward0]
	140596158334048 -> 140596158333760
	140596200688816 [label="conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	140596200688816 -> 140596158334048
	140596158334048 [label=AccumulateGrad]
	140596158333712 -> 140596158333616
	140596200688912 [label="bn1.weight
 (64)" fillcolor=lightblue]
	140596200688912 -> 140596158333712
	140596158333712 [label=AccumulateGrad]
	140596158333856 -> 140596158333616
	140596200689008 [label="bn1.bias
 (64)" fillcolor=lightblue]
	140596200689008 -> 140596158333856
	140596158333856 [label=AccumulateGrad]
	140596158333376 -> 140596158333136
	140596200689968 [label="layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	140596200689968 -> 140596158333376
	140596158333376 [label=AccumulateGrad]
	140596158332992 -> 140596158333088
	140596200690064 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	140596200690064 -> 140596158332992
	140596158332992 [label=AccumulateGrad]
	140596158333232 -> 140596158333088
	140596200690160 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	140596200690160 -> 140596158333232
	140596158333232 [label=AccumulateGrad]
	140596158644128 -> 140596158643888
	140596200690544 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140596200690544 -> 140596158644128
	140596158644128 [label=AccumulateGrad]
	140596158643744 -> 140596158643840
	140596200690640 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	140596200690640 -> 140596158643744
	140596158643744 [label=AccumulateGrad]
	140596158643984 -> 140596158643840
	140596200690736 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	140596200690736 -> 140596158643984
	140596158643984 [label=AccumulateGrad]
	140596158643648 -> 140596158643504
	140596200691120 [label="layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	140596200691120 -> 140596158643648
	140596158643648 [label=AccumulateGrad]
	140596158643456 -> 140596158643264
	140596200691216 [label="layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	140596200691216 -> 140596158643456
	140596158643456 [label=AccumulateGrad]
	140596158643408 -> 140596158643264
	140596200691312 [label="layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	140596200691312 -> 140596158643408
	140596158643408 [label=AccumulateGrad]
	140596158643024 -> 140596158643216
	140596158643024 [label=CudnnBatchNormBackward0]
	140596158644080 -> 140596158643024
	140596158644080 [label=ConvolutionBackward0]
	140596158333424 -> 140596158644080
	140596158643792 -> 140596158644080
	140596200689392 [label="layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	140596200689392 -> 140596158643792
	140596158643792 [label=AccumulateGrad]
	140596158643600 -> 140596158643024
	140596200689488 [label="layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	140596200689488 -> 140596158643600
	140596158643600 [label=AccumulateGrad]
	140596158643552 -> 140596158643024
	140596200689584 [label="layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	140596200689584 -> 140596158643552
	140596158643552 [label=AccumulateGrad]
	140596158643120 -> 140596158642832
	140596200691696 [label="layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	140596200691696 -> 140596158643120
	140596158643120 [label=AccumulateGrad]
	140596158642688 -> 140596158642784
	140596200691792 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	140596200691792 -> 140596158642688
	140596158642688 [label=AccumulateGrad]
	140596158642928 -> 140596158642784
	140596200691888 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	140596200691888 -> 140596158642928
	140596158642928 [label=AccumulateGrad]
	140596158642592 -> 140596158642352
	140596200692272 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140596200692272 -> 140596158642592
	140596158642592 [label=AccumulateGrad]
	140596158642208 -> 140596158642304
	140596200692368 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	140596200692368 -> 140596158642208
	140596158642208 [label=AccumulateGrad]
	140596158642448 -> 140596158642304
	140596200692464 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	140596200692464 -> 140596158642448
	140596158642448 [label=AccumulateGrad]
	140596158642112 -> 140596158641968
	140596200692848 [label="layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	140596200692848 -> 140596158642112
	140596158642112 [label=AccumulateGrad]
	140596158641920 -> 140596158641728
	140596200692944 [label="layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	140596200692944 -> 140596158641920
	140596158641920 [label=AccumulateGrad]
	140596158641872 -> 140596158641728
	140596200693040 [label="layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	140596200693040 -> 140596158641872
	140596158641872 [label=AccumulateGrad]
	140596158641488 -> 140596158641680
	140596158641584 -> 140596158641296
	140596200693424 [label="layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	140596200693424 -> 140596158641584
	140596158641584 [label=AccumulateGrad]
	140596158641152 -> 140596158641248
	140596200693520 [label="layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	140596200693520 -> 140596158641152
	140596158641152 [label=AccumulateGrad]
	140596158641392 -> 140596158641248
	140596200693616 [label="layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	140596200693616 -> 140596158641392
	140596158641392 [label=AccumulateGrad]
	140596158641056 -> 140596158640816
	140596200694000 [label="layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140596200694000 -> 140596158641056
	140596158641056 [label=AccumulateGrad]
	140596158640672 -> 140596158640768
	140596200694096 [label="layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	140596200694096 -> 140596158640672
	140596158640672 [label=AccumulateGrad]
	140596158640912 -> 140596158640768
	140596200694192 [label="layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	140596200694192 -> 140596158640912
	140596158640912 [label=AccumulateGrad]
	140596158640576 -> 140596158640432
	140596200694576 [label="layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	140596200694576 -> 140596158640576
	140596158640576 [label=AccumulateGrad]
	140596158640384 -> 140596158640192
	140596200694672 [label="layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	140596200694672 -> 140596158640384
	140596158640384 [label=AccumulateGrad]
	140596158640336 -> 140596158640192
	140596200694768 [label="layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	140596200694768 -> 140596158640336
	140596158640336 [label=AccumulateGrad]
	140596158640048 -> 140596158640144
	140596158639952 -> 140596158639712
	140596200695728 [label="layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	140596200695728 -> 140596158639952
	140596158639952 [label=AccumulateGrad]
	140596158639568 -> 140596158639664
	140596200695824 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	140596200695824 -> 140596158639568
	140596158639568 [label=AccumulateGrad]
	140596158639808 -> 140596158639664
	140596200695920 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	140596200695920 -> 140596158639808
	140596158639808 [label=AccumulateGrad]
	140596158639472 -> 140596158639232
	140596200696304 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140596200696304 -> 140596158639472
	140596158639472 [label=AccumulateGrad]
	140596158639088 -> 140596158639184
	140596200696400 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	140596200696400 -> 140596158639088
	140596158639088 [label=AccumulateGrad]
	140596158639328 -> 140596158639184
	140596200696496 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	140596200696496 -> 140596158639328
	140596158639328 [label=AccumulateGrad]
	140596158638992 -> 140596158638848
	140596200696880 [label="layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	140596200696880 -> 140596158638992
	140596158638992 [label=AccumulateGrad]
	140596158638800 -> 140596158638608
	140596200696976 [label="layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	140596200696976 -> 140596158638800
	140596158638800 [label=AccumulateGrad]
	140596158638752 -> 140596158638608
	140596200697072 [label="layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	140596200697072 -> 140596158638752
	140596158638752 [label=AccumulateGrad]
	140596158638368 -> 140596158638560
	140596158638368 [label=CudnnBatchNormBackward0]
	140596158639424 -> 140596158638368
	140596158639424 [label=ConvolutionBackward0]
	140596158640000 -> 140596158639424
	140596158639856 -> 140596158639424
	140596200695152 [label="layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	140596200695152 -> 140596158639856
	140596158639856 [label=AccumulateGrad]
	140596158638944 -> 140596158638368
	140596200695248 [label="layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	140596200695248 -> 140596158638944
	140596158638944 [label=AccumulateGrad]
	140596158638896 -> 140596158638368
	140596200695344 [label="layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	140596200695344 -> 140596158638896
	140596158638896 [label=AccumulateGrad]
	140596158638464 -> 140596158638176
	140596200697456 [label="layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	140596200697456 -> 140596158638464
	140596158638464 [label=AccumulateGrad]
	140596158638032 -> 140596158638128
	140596200697552 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	140596200697552 -> 140596158638032
	140596158638032 [label=AccumulateGrad]
	140596158638272 -> 140596158638128
	140596200697648 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	140596200697648 -> 140596158638272
	140596158638272 [label=AccumulateGrad]
	140596158637936 -> 140596158637696
	140596200698032 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140596200698032 -> 140596158637936
	140596158637936 [label=AccumulateGrad]
	140596158637552 -> 140596158637648
	140596200698128 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	140596200698128 -> 140596158637552
	140596158637552 [label=AccumulateGrad]
	140596158637792 -> 140596158637648
	140596200698224 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	140596200698224 -> 140596158637792
	140596158637792 [label=AccumulateGrad]
	140596158637456 -> 140596158637312
	140596200698608 [label="layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	140596200698608 -> 140596158637456
	140596158637456 [label=AccumulateGrad]
	140596158637264 -> 140596158637072
	140596200698704 [label="layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	140596200698704 -> 140596158637264
	140596158637264 [label=AccumulateGrad]
	140596158637216 -> 140596158637072
	140596200698800 [label="layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	140596200698800 -> 140596158637216
	140596158637216 [label=AccumulateGrad]
	140596158636832 -> 140596158637024
	140596158636928 -> 140596158636640
	140596200699184 [label="layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	140596200699184 -> 140596158636928
	140596158636928 [label=AccumulateGrad]
	140596158636496 -> 140596158636592
	140596200699280 [label="layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	140596200699280 -> 140596158636496
	140596158636496 [label=AccumulateGrad]
	140596158636736 -> 140596158636592
	140596200699376 [label="layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	140596200699376 -> 140596158636736
	140596158636736 [label=AccumulateGrad]
	140596158636400 -> 140596158636160
	140596200699760 [label="layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140596200699760 -> 140596158636400
	140596158636400 [label=AccumulateGrad]
	140596158636016 -> 140596158636112
	140596200699856 [label="layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	140596200699856 -> 140596158636016
	140596158636016 [label=AccumulateGrad]
	140596158636256 -> 140596158636112
	140596200699952 [label="layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	140596200699952 -> 140596158636256
	140596158636256 [label=AccumulateGrad]
	140596158635920 -> 140596158635776
	140596200700336 [label="layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	140596200700336 -> 140596158635920
	140596158635920 [label=AccumulateGrad]
	140596158635728 -> 140596158635536
	140596200700432 [label="layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	140596200700432 -> 140596158635728
	140596158635728 [label=AccumulateGrad]
	140596158635680 -> 140596158635536
	140596200700528 [label="layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	140596200700528 -> 140596158635680
	140596158635680 [label=AccumulateGrad]
	140596158635296 -> 140596158635488
	140596158635392 -> 140596158635104
	140596200700912 [label="layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	140596200700912 -> 140596158635392
	140596158635392 [label=AccumulateGrad]
	140596158634960 -> 140596158635056
	140596200701008 [label="layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	140596200701008 -> 140596158634960
	140596158634960 [label=AccumulateGrad]
	140596158635200 -> 140596158635056
	140596200701104 [label="layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	140596200701104 -> 140596158635200
	140596158635200 [label=AccumulateGrad]
	140596158634864 -> 140596158634624
	140596200701488 [label="layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140596200701488 -> 140596158634864
	140596158634864 [label=AccumulateGrad]
	140596158634480 -> 140596158634576
	140596200701584 [label="layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	140596200701584 -> 140596158634480
	140596158634480 [label=AccumulateGrad]
	140596158634720 -> 140596158634576
	140596200701680 [label="layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	140596200701680 -> 140596158634720
	140596158634720 [label=AccumulateGrad]
	140596158634384 -> 140596158634240
	140595830292656 [label="layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	140595830292656 -> 140596158634384
	140596158634384 [label=AccumulateGrad]
	140596158634192 -> 140596158634000
	140595830292752 [label="layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	140595830292752 -> 140596158634192
	140596158634192 [label=AccumulateGrad]
	140596158634144 -> 140596158634000
	140595830292848 [label="layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	140595830292848 -> 140596158634144
	140596158634144 [label=AccumulateGrad]
	140596158633856 -> 140596158633952
	140596158633760 -> 140596158633520
	140595830293808 [label="layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	140595830293808 -> 140596158633760
	140596158633760 [label=AccumulateGrad]
	140596158633376 -> 140596158633472
	140595830293904 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	140595830293904 -> 140596158633376
	140596158633376 [label=AccumulateGrad]
	140596158633616 -> 140596158633472
	140595830294000 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	140595830294000 -> 140596158633616
	140596158633616 [label=AccumulateGrad]
	140596158633280 -> 140596158633040
	140595830294384 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140595830294384 -> 140596158633280
	140596158633280 [label=AccumulateGrad]
	140596158632896 -> 140596158632992
	140595830294480 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	140595830294480 -> 140596158632896
	140596158632896 [label=AccumulateGrad]
	140596158633136 -> 140596158632992
	140595830294576 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	140595830294576 -> 140596158633136
	140596158633136 [label=AccumulateGrad]
	140596158632800 -> 140596158632656
	140595830294960 [label="layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	140595830294960 -> 140596158632800
	140596158632800 [label=AccumulateGrad]
	140596158632608 -> 140596158632416
	140595830295056 [label="layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	140595830295056 -> 140596158632608
	140596158632608 [label=AccumulateGrad]
	140596158632560 -> 140596158632416
	140595830295152 [label="layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	140595830295152 -> 140596158632560
	140596158632560 [label=AccumulateGrad]
	140596158632176 -> 140596158632368
	140596158632176 [label=CudnnBatchNormBackward0]
	140596158633232 -> 140596158632176
	140596158633232 [label=ConvolutionBackward0]
	140596158633808 -> 140596158633232
	140596158633664 -> 140596158633232
	140595830293232 [label="layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	140595830293232 -> 140596158633664
	140596158633664 [label=AccumulateGrad]
	140596158632752 -> 140596158632176
	140595830293328 [label="layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	140595830293328 -> 140596158632752
	140596158632752 [label=AccumulateGrad]
	140596158632704 -> 140596158632176
	140595830293424 [label="layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	140595830293424 -> 140596158632704
	140596158632704 [label=AccumulateGrad]
	140596158632272 -> 140596158631984
	140595830295536 [label="layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	140595830295536 -> 140596158632272
	140596158632272 [label=AccumulateGrad]
	140596158631840 -> 140596158631936
	140595830295632 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	140595830295632 -> 140596158631840
	140596158631840 [label=AccumulateGrad]
	140596158632080 -> 140596158631936
	140595830295728 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	140595830295728 -> 140596158632080
	140596158632080 [label=AccumulateGrad]
	140596158631744 -> 140596158631600
	140595830296112 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140595830296112 -> 140596158631744
	140596158631744 [label=AccumulateGrad]
	140596158631552 -> 140596158631504
	140595830296208 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	140595830296208 -> 140596158631552
	140596158631552 [label=AccumulateGrad]
	140596158631408 -> 140596158631504
	140595830296304 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	140595830296304 -> 140596158631408
	140596158631408 [label=AccumulateGrad]
	140596158631312 -> 140596158631168
	140595830296688 [label="layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	140595830296688 -> 140596158631312
	140596158631312 [label=AccumulateGrad]
	140596158631120 -> 140596158631024
	140595830296784 [label="layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	140595830296784 -> 140596158631120
	140596158631120 [label=AccumulateGrad]
	140596158631072 -> 140596158631024
	140595830296880 [label="layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	140595830296880 -> 140596158631072
	140596158631072 [label=AccumulateGrad]
	140596158630976 -> 140596158630928
	140596158630832 -> 140596158630640
	140595830297264 [label="layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	140595830297264 -> 140596158630832
	140596158630832 [label=AccumulateGrad]
	140596158630592 -> 140596158630544
	140595830297360 [label="layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	140595830297360 -> 140596158630592
	140596158630592 [label=AccumulateGrad]
	140596158630448 -> 140596158630544
	140595830297456 [label="layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	140595830297456 -> 140596158630448
	140596158630448 [label=AccumulateGrad]
	140596158630352 -> 140596158630208
	140595830297840 [label="layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140595830297840 -> 140596158630352
	140596158630352 [label=AccumulateGrad]
	140596158630160 -> 140596158630112
	140595830297936 [label="layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	140595830297936 -> 140596158630160
	140596158630160 [label=AccumulateGrad]
	140596158630016 -> 140596158630112
	140595830298032 [label="layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	140595830298032 -> 140596158630016
	140596158630016 [label=AccumulateGrad]
	140596158629920 -> 140596158629776
	140595830298416 [label="layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	140595830298416 -> 140596158629920
	140596158629920 [label=AccumulateGrad]
	140596158629728 -> 140596158629632
	140595830298512 [label="layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	140595830298512 -> 140596158629728
	140596158629728 [label=AccumulateGrad]
	140596158629680 -> 140596158629632
	140595830298608 [label="layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	140595830298608 -> 140596158629680
	140596158629680 [label=AccumulateGrad]
	140596158629584 -> 140596158629536
	140596158629440 -> 140596158629248
	140595830298992 [label="layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	140595830298992 -> 140596158629440
	140596158629440 [label=AccumulateGrad]
	140596158629200 -> 140596158629152
	140595830299088 [label="layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	140595830299088 -> 140596158629200
	140596158629200 [label=AccumulateGrad]
	140596158629056 -> 140596158629152
	140595830299184 [label="layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	140595830299184 -> 140596158629056
	140596158629056 [label=AccumulateGrad]
	140596158628960 -> 140596158628816
	140595830299568 [label="layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140595830299568 -> 140596158628960
	140596158628960 [label=AccumulateGrad]
	140596158628768 -> 140596158628720
	140595830299664 [label="layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	140595830299664 -> 140596158628768
	140596158628768 [label=AccumulateGrad]
	140596158628624 -> 140596158628720
	140595830299760 [label="layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	140595830299760 -> 140596158628624
	140596158628624 [label=AccumulateGrad]
	140596158628528 -> 140596158628384
	140595830300048 [label="layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	140595830300048 -> 140596158628528
	140596158628528 [label=AccumulateGrad]
	140596158628336 -> 140596158628240
	140595830300144 [label="layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	140595830300144 -> 140596158628336
	140596158628336 [label=AccumulateGrad]
	140596158628288 -> 140596158628240
	140595830300240 [label="layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	140595830300240 -> 140596158628288
	140596158628288 [label=AccumulateGrad]
	140596158628192 -> 140596158628144
	140596158628048 -> 140596165869520
	140595830300624 [label="layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	140595830300624 -> 140596158628048
	140596158628048 [label=AccumulateGrad]
	140596165869472 -> 140596165869424
	140595830300720 [label="layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	140595830300720 -> 140596165869472
	140596165869472 [label=AccumulateGrad]
	140596165869328 -> 140596165869424
	140595830300816 [label="layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	140595830300816 -> 140596165869328
	140596165869328 [label=AccumulateGrad]
	140596165869232 -> 140596165869088
	140595830301200 [label="layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140595830301200 -> 140596165869232
	140596165869232 [label=AccumulateGrad]
	140596165869040 -> 140596165868992
	140595830301296 [label="layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	140595830301296 -> 140596165869040
	140596165869040 [label=AccumulateGrad]
	140596165868896 -> 140596165868992
	140595830301392 [label="layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	140595830301392 -> 140596165868896
	140596165868896 [label=AccumulateGrad]
	140596165868800 -> 140596165868656
	140595830301776 [label="layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	140595830301776 -> 140596165868800
	140596165868800 [label=AccumulateGrad]
	140596165868608 -> 140596165868512
	140595830301872 [label="layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	140595830301872 -> 140596165868608
	140596165868608 [label=AccumulateGrad]
	140596165868560 -> 140596165868512
	140595830301968 [label="layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	140595830301968 -> 140596165868560
	140596165868560 [label=AccumulateGrad]
	140596165868464 -> 140596165868416
	140596165868320 -> 140596165868128
	140595830302352 [label="layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	140595830302352 -> 140596165868320
	140596165868320 [label=AccumulateGrad]
	140596165868080 -> 140596165868032
	140595830302448 [label="layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	140595830302448 -> 140596165868080
	140596165868080 [label=AccumulateGrad]
	140596165867936 -> 140596165868032
	140595830302544 [label="layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	140595830302544 -> 140596165867936
	140596165867936 [label=AccumulateGrad]
	140596165867840 -> 140596165867696
	140595830302928 [label="layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140595830302928 -> 140596165867840
	140596165867840 [label=AccumulateGrad]
	140596165867648 -> 140596165867600
	140595830303024 [label="layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	140595830303024 -> 140596165867648
	140596165867648 [label=AccumulateGrad]
	140596165867504 -> 140596165867600
	140595830303120 [label="layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	140595830303120 -> 140596165867504
	140596165867504 [label=AccumulateGrad]
	140596165867408 -> 140596165867264
	140595830303504 [label="layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	140595830303504 -> 140596165867408
	140596165867408 [label=AccumulateGrad]
	140596165867216 -> 140596165867120
	140595830303600 [label="layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	140595830303600 -> 140596165867216
	140596165867216 [label=AccumulateGrad]
	140596165867168 -> 140596165867120
	140595830303696 [label="layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	140595830303696 -> 140596165867168
	140596165867168 [label=AccumulateGrad]
	140596165867072 -> 140596165867024
	140596165866832 -> 140596165866688
	140595830304656 [label="layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	140595830304656 -> 140596165866832
	140596165866832 [label=AccumulateGrad]
	140596165866640 -> 140596165866592
	140595830304752 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	140595830304752 -> 140596165866640
	140596165866640 [label=AccumulateGrad]
	140596165866496 -> 140596165866592
	140595830304848 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	140595830304848 -> 140596165866496
	140596165866496 [label=AccumulateGrad]
	140596165866400 -> 140596165866256
	140595830305232 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	140595830305232 -> 140596165866400
	140596165866400 [label=AccumulateGrad]
	140596165866208 -> 140596165866160
	140595830305328 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	140595830305328 -> 140596165866208
	140596165866208 [label=AccumulateGrad]
	140596165866064 -> 140596165866160
	140595830305424 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	140595830305424 -> 140596165866064
	140596165866064 [label=AccumulateGrad]
	140596165865968 -> 140596165865824
	140595830305808 [label="layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	140595830305808 -> 140596165865968
	140596165865968 [label=AccumulateGrad]
	140596165865776 -> 140596165865680
	140595830305904 [label="layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	140595830305904 -> 140596165865776
	140596165865776 [label=AccumulateGrad]
	140596165865728 -> 140596165865680
	140595830306000 [label="layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	140595830306000 -> 140596165865728
	140596165865728 [label=AccumulateGrad]
	140596165865632 -> 140596165865584
	140596165865632 [label=CudnnBatchNormBackward0]
	140596165866352 -> 140596165865632
	140596165866352 [label=ConvolutionBackward0]
	140596165866880 -> 140596165866352
	140596165866736 -> 140596165866352
	140595830304080 [label="layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	140595830304080 -> 140596165866736
	140596165866736 [label=AccumulateGrad]
	140596165865920 -> 140596165865632
	140595830304176 [label="layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	140595830304176 -> 140596165865920
	140596165865920 [label=AccumulateGrad]
	140596165865872 -> 140596165865632
	140595830304272 [label="layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	140595830304272 -> 140596165865872
	140596165865872 [label=AccumulateGrad]
	140596165865488 -> 140596165865296
	140595830306384 [label="layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	140595830306384 -> 140596165865488
	140596165865488 [label=AccumulateGrad]
	140596165865248 -> 140596165865200
	140595830306480 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	140595830306480 -> 140596165865248
	140596165865248 [label=AccumulateGrad]
	140596165865104 -> 140596165865200
	140595830306576 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	140595830306576 -> 140596165865104
	140596165865104 [label=AccumulateGrad]
	140596165865008 -> 140596165864864
	140595830306960 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	140595830306960 -> 140596165865008
	140596165865008 [label=AccumulateGrad]
	140596165864816 -> 140596165864768
	140595830307056 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	140595830307056 -> 140596165864816
	140596165864816 [label=AccumulateGrad]
	140596165864672 -> 140596165864768
	140595830307152 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	140595830307152 -> 140596165864672
	140596165864672 [label=AccumulateGrad]
	140596165864576 -> 140596165864432
	140595830307536 [label="layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	140595830307536 -> 140596165864576
	140596165864576 [label=AccumulateGrad]
	140596165864384 -> 140596165864288
	140595830307632 [label="layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	140595830307632 -> 140596165864384
	140596165864384 [label=AccumulateGrad]
	140596165864336 -> 140596165864288
	140595830307728 [label="layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	140595830307728 -> 140596165864336
	140596165864336 [label=AccumulateGrad]
	140596165864240 -> 140596165864192
	140596165864096 -> 140596165863904
	140595830308112 [label="layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	140595830308112 -> 140596165864096
	140596165864096 [label=AccumulateGrad]
	140596165863856 -> 140596165863808
	140595830308208 [label="layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	140595830308208 -> 140596165863856
	140596165863856 [label=AccumulateGrad]
	140596165863712 -> 140596165863808
	140595830308304 [label="layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	140595830308304 -> 140596165863712
	140596165863712 [label=AccumulateGrad]
	140596165863616 -> 140596165863472
	140595830308688 [label="layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	140595830308688 -> 140596165863616
	140596165863616 [label=AccumulateGrad]
	140596165863424 -> 140596165863376
	140595830308784 [label="layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	140595830308784 -> 140596165863424
	140596165863424 [label=AccumulateGrad]
	140596165863280 -> 140596165863376
	140596165673040 [label="layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	140596165673040 -> 140596165863280
	140596165863280 [label=AccumulateGrad]
	140596165863088 -> 140596165859008
	140596165673424 [label="layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	140596165673424 -> 140596165863088
	140596165863088 [label=AccumulateGrad]
	140596165854928 -> 140596165860976
	140596165673520 [label="layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	140596165673520 -> 140596165854928
	140596165854928 [label=AccumulateGrad]
	140596165854112 -> 140596165860976
	140596165673616 [label="layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	140596165673616 -> 140596165854112
	140596165854112 [label=AccumulateGrad]
	140596165862944 -> 140596165862896
	140596165861504 -> 140596165862032
	140596165861504 [label=TBackward0]
	140596165862848 -> 140596165861504
	140596165674000 [label="fc.weight
 (1000, 2048)" fillcolor=lightblue]
	140596165674000 -> 140596165862848
	140596165862848 [label=AccumulateGrad]
	140596165862032 -> 140595792196624
}
