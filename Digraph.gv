digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140641151682320 [label="
 (1, 10)" fillcolor=darkolivegreen1]
	140641151128848 [label=AddmmBackward0]
	140641151129712 -> 140641151128848
	140641151336368 [label="fc.bias
 (10)" fillcolor=lightblue]
	140641151336368 -> 140641151129712
	140641151129712 [label=AccumulateGrad]
	140641151131152 -> 140641151128848
	140641151131152 [label=ViewBackward0]
	140641151127696 -> 140641151131152
	140641151127696 [label=MeanBackward1]
	140641151128080 -> 140641151127696
	140641151128080 [label=ReluBackward0]
	140641151128368 -> 140641151128080
	140641151128368 [label=AddBackward0]
	140641151129616 -> 140641151128368
	140641151129616 [label=NativeBatchNormBackward0]
	140641151132208 -> 140641151129616
	140641151132208 [label=ConvolutionBackward0]
	140641151131536 -> 140641151132208
	140641151131536 [label=ReluBackward0]
	140641151132112 -> 140641151131536
	140641151132112 [label=NativeBatchNormBackward0]
	140641151127888 -> 140641151132112
	140641151127888 [label=ConvolutionBackward0]
	140641151130000 -> 140641151127888
	140641151130000 [label=ReluBackward0]
	140641151127120 -> 140641151130000
	140641151127120 [label=AddBackward0]
	140641151126928 -> 140641151127120
	140641151126928 [label=NativeBatchNormBackward0]
	140641151126640 -> 140641151126928
	140641151126640 [label=ConvolutionBackward0]
	140641151126256 -> 140641151126640
	140641151126256 [label=ReluBackward0]
	140641151125968 -> 140641151126256
	140641151125968 [label=NativeBatchNormBackward0]
	140641151125776 -> 140641151125968
	140641151125776 [label=ConvolutionBackward0]
	140641151125392 -> 140641151125776
	140641151125392 [label=ReluBackward0]
	140641151125200 -> 140641151125392
	140641151125200 [label=AddBackward0]
	140641151125104 -> 140641151125200
	140641151125104 [label=NativeBatchNormBackward0]
	140641151124960 -> 140641151125104
	140641151124960 [label=ConvolutionBackward0]
	140641151124768 -> 140641151124960
	140641151124768 [label=ReluBackward0]
	140641151124624 -> 140641151124768
	140641151124624 [label=NativeBatchNormBackward0]
	140641151124576 -> 140641151124624
	140641151124576 [label=ConvolutionBackward0]
	140641151125152 -> 140641151124576
	140641151125152 [label=ReluBackward0]
	140641210150256 -> 140641151125152
	140641210150256 [label=AddBackward0]
	140641210150064 -> 140641210150256
	140641210150064 [label=NativeBatchNormBackward0]
	140641210149776 -> 140641210150064
	140641210149776 [label=ConvolutionBackward0]
	140641210149392 -> 140641210149776
	140641210149392 [label=ReluBackward0]
	140641210149104 -> 140641210149392
	140641210149104 [label=NativeBatchNormBackward0]
	140641210148912 -> 140641210149104
	140641210148912 [label=ConvolutionBackward0]
	140641210148528 -> 140641210148912
	140641210148528 [label=ReluBackward0]
	140641210148240 -> 140641210148528
	140641210148240 [label=AddBackward0]
	140641210148048 -> 140641210148240
	140641210148048 [label=NativeBatchNormBackward0]
	140641210147760 -> 140641210148048
	140641210147760 [label=ConvolutionBackward0]
	140641210147376 -> 140641210147760
	140641210147376 [label=ReluBackward0]
	140641210147088 -> 140641210147376
	140641210147088 [label=NativeBatchNormBackward0]
	140641210146896 -> 140641210147088
	140641210146896 [label=ConvolutionBackward0]
	140641210148144 -> 140641210146896
	140641210148144 [label=ReluBackward0]
	140641210146320 -> 140641210148144
	140641210146320 [label=AddBackward0]
	140641210146128 -> 140641210146320
	140641210146128 [label=NativeBatchNormBackward0]
	140641210145840 -> 140641210146128
	140641210145840 [label=ConvolutionBackward0]
	140641210145456 -> 140641210145840
	140641210145456 [label=ReluBackward0]
	140641210145168 -> 140641210145456
	140641210145168 [label=NativeBatchNormBackward0]
	140641210144976 -> 140641210145168
	140641210144976 [label=ConvolutionBackward0]
	140641210144592 -> 140641210144976
	140641210144592 [label=ReluBackward0]
	140641210144304 -> 140641210144592
	140641210144304 [label=AddBackward0]
	140641210144112 -> 140641210144304
	140641210144112 [label=NativeBatchNormBackward0]
	140641210143824 -> 140641210144112
	140641210143824 [label=ConvolutionBackward0]
	140641210143440 -> 140641210143824
	140641210143440 [label=ReluBackward0]
	140641210143152 -> 140641210143440
	140641210143152 [label=NativeBatchNormBackward0]
	140641210142960 -> 140641210143152
	140641210142960 [label=ConvolutionBackward0]
	140641210144208 -> 140641210142960
	140641210144208 [label=ReluBackward0]
	140641210142384 -> 140641210144208
	140641210142384 [label=AddBackward0]
	140641210142192 -> 140641210142384
	140641210142192 [label=NativeBatchNormBackward0]
	140641210141904 -> 140641210142192
	140641210141904 [label=ConvolutionBackward0]
	140641210141520 -> 140641210141904
	140641210141520 [label=ReluBackward0]
	140641210141232 -> 140641210141520
	140641210141232 [label=NativeBatchNormBackward0]
	140641210141040 -> 140641210141232
	140641210141040 [label=ConvolutionBackward0]
	140641210142288 -> 140641210141040
	140641210142288 [label=MaxPool2DWithIndicesBackward0]
	140641210140464 -> 140641210142288
	140641210140464 [label=ReluBackward0]
	140641210140272 -> 140641210140464
	140641210140272 [label=NativeBatchNormBackward0]
	140641210140080 -> 140641210140272
	140641210140080 [label=ConvolutionBackward0]
	140641210139696 -> 140641210140080
	140641151084272 [label="conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	140641151084272 -> 140641210139696
	140641210139696 [label=AccumulateGrad]
	140641210140176 -> 140641210140272
	140641151084176 [label="bn1.weight
 (64)" fillcolor=lightblue]
	140641151084176 -> 140641210140176
	140641210140176 [label=AccumulateGrad]
	140641210140848 -> 140641210140272
	140641151084080 [label="bn1.bias
 (64)" fillcolor=lightblue]
	140641151084080 -> 140641210140848
	140641210140848 [label=AccumulateGrad]
	140641210140656 -> 140641210141040
	140641151083696 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140641151083696 -> 140641210140656
	140641210140656 [label=AccumulateGrad]
	140641210141136 -> 140641210141232
	140641151083600 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	140641151083600 -> 140641210141136
	140641210141136 [label=AccumulateGrad]
	140641210141424 -> 140641210141232
	140641151083504 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	140641151083504 -> 140641210141424
	140641210141424 [label=AccumulateGrad]
	140641210141616 -> 140641210141904
	140641151083120 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140641151083120 -> 140641210141616
	140641210141616 [label=AccumulateGrad]
	140641210142000 -> 140641210142192
	140641151088208 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	140641151088208 -> 140641210142000
	140641210142000 [label=AccumulateGrad]
	140641210142096 -> 140641210142192
	140641151087824 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	140641151087824 -> 140641210142096
	140641210142096 [label=AccumulateGrad]
	140641210142288 -> 140641210142384
	140641210142576 -> 140641210142960
	140641151082832 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140641151082832 -> 140641210142576
	140641210142576 [label=AccumulateGrad]
	140641210143056 -> 140641210143152
	140641151082736 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	140641151082736 -> 140641210143056
	140641210143056 [label=AccumulateGrad]
	140641210143344 -> 140641210143152
	140641151082640 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	140641151082640 -> 140641210143344
	140641210143344 [label=AccumulateGrad]
	140641210143536 -> 140641210143824
	140641151082256 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140641151082256 -> 140641210143536
	140641210143536 [label=AccumulateGrad]
	140641210143920 -> 140641210144112
	140641151082160 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	140641151082160 -> 140641210143920
	140641210143920 [label=AccumulateGrad]
	140641210144016 -> 140641210144112
	140641151082064 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	140641151082064 -> 140641210144016
	140641210144016 [label=AccumulateGrad]
	140641210144208 -> 140641210144304
	140641210144688 -> 140641210144976
	140641151081104 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	140641151081104 -> 140641210144688
	140641210144688 [label=AccumulateGrad]
	140641210145072 -> 140641210145168
	140641151081008 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	140641151081008 -> 140641210145072
	140641210145072 [label=AccumulateGrad]
	140641210145360 -> 140641210145168
	140641151080912 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	140641151080912 -> 140641210145360
	140641210145360 [label=AccumulateGrad]
	140641210145552 -> 140641210145840
	140641150750352 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140641150750352 -> 140641210145552
	140641210145552 [label=AccumulateGrad]
	140641210145936 -> 140641210146128
	140641150750256 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	140641150750256 -> 140641210145936
	140641210145936 [label=AccumulateGrad]
	140641210146032 -> 140641210146128
	140641150750160 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	140641150750160 -> 140641210146032
	140641210146032 [label=AccumulateGrad]
	140641210146224 -> 140641210146320
	140641210146224 [label=NativeBatchNormBackward0]
	140641210144784 -> 140641210146224
	140641210144784 [label=ConvolutionBackward0]
	140641210144592 -> 140641210144784
	140641210144496 -> 140641210144784
	140641151081680 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	140641151081680 -> 140641210144496
	140641210144496 [label=AccumulateGrad]
	140641210145648 -> 140641210146224
	140641151081584 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	140641151081584 -> 140641210145648
	140641210145648 [label=AccumulateGrad]
	140641210145744 -> 140641210146224
	140641151081488 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	140641151081488 -> 140641210145744
	140641210145744 [label=AccumulateGrad]
	140641210146512 -> 140641210146896
	140641150749776 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140641150749776 -> 140641210146512
	140641210146512 [label=AccumulateGrad]
	140641210146992 -> 140641210147088
	140641150749680 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	140641150749680 -> 140641210146992
	140641210146992 [label=AccumulateGrad]
	140641210147280 -> 140641210147088
	140641150749584 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	140641150749584 -> 140641210147280
	140641210147280 [label=AccumulateGrad]
	140641210147472 -> 140641210147760
	140641150749200 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140641150749200 -> 140641210147472
	140641210147472 [label=AccumulateGrad]
	140641210147856 -> 140641210148048
	140641150749104 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	140641150749104 -> 140641210147856
	140641210147856 [label=AccumulateGrad]
	140641210147952 -> 140641210148048
	140641150749008 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	140641150749008 -> 140641210147952
	140641210147952 [label=AccumulateGrad]
	140641210148144 -> 140641210148240
	140641210148624 -> 140641210148912
	140641150748048 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	140641150748048 -> 140641210148624
	140641210148624 [label=AccumulateGrad]
	140641210149008 -> 140641210149104
	140641150747952 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	140641150747952 -> 140641210149008
	140641210149008 [label=AccumulateGrad]
	140641210149296 -> 140641210149104
	140641150747856 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	140641150747856 -> 140641210149296
	140641210149296 [label=AccumulateGrad]
	140641210149488 -> 140641210149776
	140641150747472 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140641150747472 -> 140641210149488
	140641210149488 [label=AccumulateGrad]
	140641210149872 -> 140641210150064
	140641150747376 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	140641150747376 -> 140641210149872
	140641210149872 [label=AccumulateGrad]
	140641210149968 -> 140641210150064
	140641150747280 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	140641150747280 -> 140641210149968
	140641210149968 [label=AccumulateGrad]
	140641210150160 -> 140641210150256
	140641210150160 [label=NativeBatchNormBackward0]
	140641210148720 -> 140641210150160
	140641210148720 [label=ConvolutionBackward0]
	140641210148528 -> 140641210148720
	140641210148432 -> 140641210148720
	140641150748624 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	140641150748624 -> 140641210148432
	140641210148432 [label=AccumulateGrad]
	140641210149584 -> 140641210150160
	140641150748528 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	140641150748528 -> 140641210149584
	140641210149584 [label=AccumulateGrad]
	140641210149680 -> 140641210150160
	140641150748432 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	140641150748432 -> 140641210149680
	140641210149680 [label=AccumulateGrad]
	140641210150448 -> 140641151124576
	140641150746896 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140641150746896 -> 140641210150448
	140641210150448 [label=AccumulateGrad]
	140641151124720 -> 140641151124624
	140641150746800 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	140641150746800 -> 140641151124720
	140641151124720 [label=AccumulateGrad]
	140641210152416 -> 140641151124624
	140641150746704 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	140641150746704 -> 140641210152416
	140641210152416 [label=AccumulateGrad]
	140641151124816 -> 140641151124960
	140641210039952 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140641210039952 -> 140641151124816
	140641151124816 [label=AccumulateGrad]
	140641151125008 -> 140641151125104
	140641210039856 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	140641210039856 -> 140641151125008
	140641151125008 [label=AccumulateGrad]
	140641151125056 -> 140641151125104
	140641210039760 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	140641210039760 -> 140641151125056
	140641151125056 [label=AccumulateGrad]
	140641151125152 -> 140641151125200
	140641151125488 -> 140641151125776
	140641210038800 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	140641210038800 -> 140641151125488
	140641151125488 [label=AccumulateGrad]
	140641151125872 -> 140641151125968
	140641210038704 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	140641210038704 -> 140641151125872
	140641151125872 [label=AccumulateGrad]
	140641151126160 -> 140641151125968
	140641210038608 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	140641210038608 -> 140641151126160
	140641151126160 [label=AccumulateGrad]
	140641151126352 -> 140641151126640
	140641210038224 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	140641210038224 -> 140641151126352
	140641151126352 [label=AccumulateGrad]
	140641151126736 -> 140641151126928
	140641210038128 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	140641210038128 -> 140641151126736
	140641151126736 [label=AccumulateGrad]
	140641151126832 -> 140641151126928
	140641210038032 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	140641210038032 -> 140641151126832
	140641151126832 [label=AccumulateGrad]
	140641151127024 -> 140641151127120
	140641151127024 [label=NativeBatchNormBackward0]
	140641151125584 -> 140641151127024
	140641151125584 [label=ConvolutionBackward0]
	140641151125392 -> 140641151125584
	140641151125296 -> 140641151125584
	140641210039376 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	140641210039376 -> 140641151125296
	140641151125296 [label=AccumulateGrad]
	140641151126448 -> 140641151127024
	140641210039280 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	140641210039280 -> 140641151126448
	140641151126448 [label=AccumulateGrad]
	140641151126544 -> 140641151127024
	140641210039184 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	140641210039184 -> 140641151126544
	140641151126544 [label=AccumulateGrad]
	140641151127312 -> 140641151127888
	140641210037648 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	140641210037648 -> 140641151127312
	140641151127312 [label=AccumulateGrad]
	140641151128656 -> 140641151132112
	140641210037552 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	140641210037552 -> 140641151128656
	140641151128656 [label=AccumulateGrad]
	140641151131824 -> 140641151132112
	140641210037456 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	140641210037456 -> 140641151131824
	140641151131824 [label=AccumulateGrad]
	140641151132496 -> 140641151132208
	140641210037072 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	140641210037072 -> 140641151132496
	140641151132496 [label=AccumulateGrad]
	140641151130864 -> 140641151129616
	140641210036976 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	140641210036976 -> 140641151130864
	140641151130864 [label=AccumulateGrad]
	140641151130576 -> 140641151129616
	140641210036880 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	140641210036880 -> 140641151130576
	140641151130576 [label=AccumulateGrad]
	140641151130000 -> 140641151128368
	140641151130672 -> 140641151128848
	140641151130672 [label=TBackward0]
	140641151128272 -> 140641151130672
	140641210036592 [label="fc.weight
 (10, 512)" fillcolor=lightblue]
	140641210036592 -> 140641151128272
	140641151128272 [label=AccumulateGrad]
	140641151128848 -> 140641151682320
}
